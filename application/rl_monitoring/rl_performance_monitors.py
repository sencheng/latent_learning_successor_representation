# basic imports
import numpy as np
import pyqtgraph as pg
                    

class RLPerformanceMonitorBaseline():
    '''
    Performance monitor. Used for tracking learning progress.
    
    | **Args**
    | rlAgent:                      Reference to the RL agent used.
    | trial:                        Maximum number of trials for which the experiment is run.
    | guiParent:                    The main window for visualization.
    | visualOutput:                 If true, the learning progress will be plotted.
    | rewardRange:                  The range for which the cumulative reward will be plotted.
    '''
    
    def __init__(self, rlAgent, trials, guiParent, visualOutput, rewardRange=[0, 1]):
        # store the rlAgent
        self.rlAgent = rlAgent
        self.guiParent = guiParent
        # shall visual output be provided?
        self.visualOutput = visualOutput
        #define the variables that will be monitored
        self.rlRewardTraceRaw = np.zeros(trials, dtype='float')
        self.rlRewardTraceRefined = np.zeros(trials, dtype='float')
        # this is the accumulation range for smoothing the reward curve
        self.accumulationRangeReward = 20
        # this is the accumulation interval for correct/incorrect decisions at the beginning/end of the single experimental phases (acquisition,extinction,renewal) 
        self.accumulationIntervalPerformance = 10
        
        if visualOutput:
            # redefine the gui's dimensions
            self.guiParent.setGeometry(50, 50, 1600, 600)
            # set up the required plots
            self.rlRewardPlot = self.guiParent.addPlot(title="Reinforcement learning progress")
            # set x/y-ranges for the plots
            self.rlRewardPlot.setXRange(0, trials)
            self.rlRewardPlot.setYRange(rewardRange[0], rewardRange[1])
            # define the episodes domain
            self.episodesDomain = np.linspace(0, trials, trials)
            # each variable has a dedicated graph that can be used for displaying the monitored values
            self.rlRewardTraceRawGraph=self.rlRewardPlot.plot(self.episodesDomain, self.rlRewardTraceRaw)
            self.rlRewardTraceRefinedGraph=self.rlRewardPlot.plot(self.episodesDomain, self.rlRewardTraceRefined)

    def clearPlots(self):
        '''
        This function clears the plots generated by the performance monitor.
        '''
        if self.visualOutput:
            self.guiParent.removeItem(self.rlRewardPlot)
    
    def update(self, trial, logs):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        | **Args**
        | trial:                        The actual trial number.
        | logs:                         Information from the reinforcement learning subsystem.
        '''
        # update the reward traces
        rlReward = logs['episode_reward']
        self.rlRewardTraceRaw[trial] = rlReward
        # prepare aggregated reward trace
        aggregatedRewardTraceRaw = None
        if trial < self.accumulationRangeReward:
            aggregatedRewardTraceRaw = self.rlRewardTraceRaw[trial:None:-1]
        else:
            aggregatedRewardTraceRaw = self.rlRewardTraceRaw[trial:trial-self.accumulationRangeReward:-1]
        self.rlRewardTraceRefined[trial] = np.mean(aggregatedRewardTraceRaw)
        
        if self.visualOutput:
            # set the graph's data
            self.rlRewardTraceRawGraph.setData(self.episodesDomain, self.rlRewardTraceRaw, pen=pg.mkPen(color=(128, 128, 128), width=1))
            self.rlRewardTraceRefinedGraph.setData(self.episodesDomain, self.rlRewardTraceRefined, pen=pg.mkPen(color=(255, 0, 0), width=2))
            
            
class CRCMonitor():
    '''
    Performance monitor. used to record and display the CRC.
    
    | **Args**
    | rlAgent:                      Reference to the RL agent used.
    | trial:                        Maximum number of trials for which the experiment is run.
    | guiParent:                    The main window for visualization.
    | visualOutput:                 If true, the learning progress will be plotted.
    '''
    
    def __init__(self, rlAgent, trials, guiParent, visualOutput):
        # store the rlAgent
        self.rlAgent = rlAgent
        self.guiParent = guiParent
        # shall visual output be provided?
        self.visualOutput = visualOutput
        #define the variables that will be monitored
        self.responses = np.zeros(trials, dtype='float')
        self.CRC = np.zeros(trials, dtype='float')
        
        if visualOutput:
            # redefine the gui's dimensions
            self.guiParent.setGeometry(50, 50, 1600, 600)
            # set up the required plots
            self.CRCPlot = self.guiParent.addPlot(title="CRC")
            # set initial x/y-ranges of the plot
            self.CRCPlot.setXRange(0, 0)
            self.CRCPlot.setYRange(0, 0)
            # define the episodes domain
            self.episodesDomain = np.linspace(0, trials, trials)
            # make graph for CRC
            self.CRCGraph = self.CRCPlot.plot(self.episodesDomain, self.CRC)

    def clearPlots(self):
        '''
        This function clears the plots generated by the performance monitor.
        '''
        if self.visualOutput:
            self.guiParent.removeItem(self.rlRewardPlot)
    
    def update(self, trial, logs):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        | **Args**
        | trial:                        The actual trial number.
        | logs:                         Information from the reinforcement learning subsystem.
        '''
        # store agent's response
        self.responses[trial] = logs['response']
        # update CRC
        self.CRC[trial] = np.sum(self.responses[:(trial + 1)])
        
        if self.visualOutput:
            # update x/y-ranges of the plot
            self.CRCPlot.setXRange(0, trial)
            self.CRCPlot.setYRange(np.amin(self.CRC), np.amax(self.CRC))
            # set the graph's data
            self.CRCGraph.setData(self.episodesDomain, self.CRC, pen=pg.mkPen(color=(128, 128, 128), width=1))